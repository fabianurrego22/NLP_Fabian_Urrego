{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSZruQHnBkEO"
   },
   "source": [
    "# Trabajo Final procesamiento de lenguaje natural (NLP) <a class=\"tocSkip\">\n",
    "## Universidad Pontificia Bolivariana <a class=\"tocSkip\">\n",
    "\n",
    "**Estudiante:** Fabian Urrego\n",
    "\n",
    "**ID:** 000059399\n",
    "\n",
    "\n",
    "\n",
    "**Subir Este notebook de Trabajo Final en su repositorio de GitHub. y enviar el link en **Microsoft TEAMS**\n",
    "\n",
    "Docente: [Jose R. Zapata](https://joserzapata.github.io)\n",
    "- https://joserzapata.github.io\n",
    "- https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo del Trabajo\n",
    "Realizar un proceso de Procesamiento de lenguaje natural (NLP) para dejar los datos preparados para ser usados con algoritmos de Machine Learning para Clasificaci√≥n como objetivo final del trabajo.\n",
    "\n",
    "los datos preparados para ser usados con algoritmos de Machine Learning para Regresi√≥n o Clasificaci√≥n como objetivo final del trabajo.\n",
    "\n",
    "El trabajo se realizara en este jupyter notebook y subirlo a su repositorio de github creado en clase. (**Recuerde poner su nombre e informaci√≥n**)\n",
    "\n",
    "## Las actividades a realizar\n",
    "    \n",
    "\n",
    "1) Limpiar los datos de texto.\n",
    "\n",
    "   - https://joserzapata.github.io/courses/nlp/procesamiento-basico/\n",
    "\n",
    "   - https://joserzapata.github.io/courses/nlp/preprocesamiento-texto/\n",
    "\n",
    "2) Realizar la representaci√≥n de texto:\n",
    "\n",
    "   - https://joserzapata.github.io/courses/nlp/representaciones/\n",
    "\n",
    "   - Tokenizaci√≥n\n",
    "   - Lematizaci√≥n o stemming\n",
    "   - Representaci√≥n de los datos de texto (Bag of Words o TF-IDF)\n",
    "\n",
    "3) Utilice un modelo de Machine Learning para clasificaci√≥n para entrenar y evaluar el modelo.\n",
    "        \n",
    "    - Calcule estas las m√©tricas de evaluaci√≥n (accuracy, precision, recall, f1-score) - https://joserzapata.github.io/courses/python-ciencia-datos/clasificacion/#evaluacion-modelo-simple\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "*NOTA: No dude en contactarme para cualquier pregunta o inquietud :) por el chat de Teams o al correo\n",
    "joser.zapata@upb.edu.co*\n",
    "\n",
    "## EVALUACI√ìN\n",
    "\n",
    "\n",
    "|Porcentaje en la evaluaci√≥n | Descripci√≥n| Nada | Incompleto | Completo \n",
    "| :---: |:---: |:---: |:---: |:---: \n",
    "| 5 % |**limpieza de los datos** |\n",
    "| 5 % | **Representacion de los datos** |\n",
    "| 5 % | **Machine Learning** <br> Entrenar y evaluar el modelo propuesto |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1784,
     "status": "ok",
     "timestamp": 1742313766605,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "FbX4vPbang_e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLpaicLXninI"
   },
   "source": [
    "Carga del dataset\n",
    "\n",
    "El archivo  se carga en aproximadamente 10 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1742313775953,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "Xh8VOY5Mnjmo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50000 entries, 118683 to 96414\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   author_review_desc  50000 non-null  object\n",
      " 1   author_rating       50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Usar su ID UPB Ejemplo: \"0028984798\"\n",
    "id_upb = \"000059399\"\n",
    "\n",
    "data_reviews = pd.read_parquet(\n",
    "    \"https://www.dropbox.com/scl/fi/gvk9yj8cn96oocr9z058x/filmaffinity_reviews_dataset.parquet?rlkey=xgvr00zvkxbkwqqavqutpsshg&st=xjb7xze9&dl=1\"\n",
    ")\n",
    "data_reviews = data_reviews.sample(n=50_000, random_state=int(id_upb))\n",
    "data_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGD-mkRRnm01"
   },
   "source": [
    "Ejemplo de algunas filas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1742313777216,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "rIsJnVvPnnvc",
    "outputId": "a97d6896-3f9b-4a02-88fc-f3e6f84d0602"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_review_desc</th>\n",
       "      <th>author_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95713</th>\n",
       "      <td>\\nPel√≠cula de ciencia ficci√≥n en la que la Tie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129952</th>\n",
       "      <td>\\nUna antigua compa√±era de trabajo me dijo que...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35628</th>\n",
       "      <td>\\n‚ÄúAs√≠ continuaron viviendo en una realidad es...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118975</th>\n",
       "      <td>\\nExitosa ficci√≥n emitida por Televisi√≥n Espa√±...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>\\nEl territorio de la f√°bula, de la par√°bola, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author_review_desc  author_rating\n",
       "95713   \\nPel√≠cula de ciencia ficci√≥n en la que la Tie...              5\n",
       "129952  \\nUna antigua compa√±era de trabajo me dijo que...              2\n",
       "35628   \\n‚ÄúAs√≠ continuaron viviendo en una realidad es...             10\n",
       "118975  \\nExitosa ficci√≥n emitida por Televisi√≥n Espa√±...              7\n",
       "20527   \\nEl territorio de la f√°bula, de la par√°bola, ...              8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKA9UPk6p7bZ"
   },
   "source": [
    "Evaluar los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742313779074,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "2wwqoksvqQFk",
    "outputId": "fea4451b-9936-4eb7-b1b9-14fd83f7c72f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_review_desc    0\n",
       "author_rating         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no hay nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar si hay valores duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reviews.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay  valores duplicados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiBcIGoZsQJC"
   },
   "source": [
    "## Clasificaci√≥n Tradicional para An√°lisis de Sentimientos y Categor√≠as üëç üëé\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPUGUs9AmA_b"
   },
   "source": [
    "Definici√≥n: Se considera rese√±a positiva cuando la puntuaci√≥n (\"author_rating\") es mayor que 6; negativa en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZTzGi7tmMBN"
   },
   "source": [
    "Crear la variable binaria de sentimiento: 1 (positivo) si author_rating > 6, 0 (negativo) de lo contrario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1742314413450,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "Wu66d05mmKtT"
   },
   "outputs": [],
   "source": [
    "UMBRAL = 6\n",
    "data_reviews[\"sentiment_bin\"] = (data_reviews[\"author_rating\"] > UMBRAL).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_data = data_reviews[\"author_review_desc\"]\n",
    "y_data = data_reviews[\"sentiment_bin\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el dataset en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.2,\n",
    "    stratify=y_data,  # Mantener la proporci√≥n de clases en ambos conjuntos\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis exploratorio de los datos\n",
    "\n",
    "para determinar que tipo de limpieza se debe realizar a los datos de texto de `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81366     \\nEn el festival Grec de Barcelona del a√±o 201...\n",
       "84717     \\nVi primero la de 1953 y luego √©sta. Mejora e...\n",
       "132850    \\nCual civilizaci√≥n condenada a la extinci√≥n, ...\n",
       "60505     \\nEn pleno siglo XXI no es de extra√±ar el topa...\n",
       "124468    \\n¬øPara qu√© reunir a todos estos actores y hac...\n",
       "                                ...                        \n",
       "99934     \\nLo que hace que la pel√≠cula funcione, y muy ...\n",
       "53796     \\nNuestra se√±ora de Atocha. No cuelgue. Condes...\n",
       "121255    \\nTres a√±os despu√©s de los sucesos de \"Jaws\", ...\n",
       "118597    \\nPel√≠cula basada en el pasado romano, buena m...\n",
       "139664    \\nTodo el argumento gira en torno al personaje...\n",
       "Name: author_review_desc, Length: 40000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Limpieza de los datos de texto\n",
    "\n",
    "Tomar los datos de `X_train` y aplicar las funciones de limpieza que considere necesarias\n",
    "\n",
    "Ayudas:\n",
    "\n",
    "- Convertir a min√∫sculas\n",
    "- Eliminar caracteres especiales y n√∫meros\n",
    "- Corregir palabras mal escritas\n",
    "- etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "LIMPIEZA DE X_TRAIN\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "üì¶ Dataset de ejemplo: 8 textos\n",
      "\n",
      "üßπ LIMPIANDO X_TRAIN\n",
      "============================================================\n",
      "Total de textos: 8\n",
      "\n",
      "\n",
      "‚úÖ Limpieza completada: 8 textos procesados\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   ‚Ä¢ Palabras originales: 65\n",
      "   ‚Ä¢ Palabras despu√©s de limpieza: 55\n",
      "   ‚Ä¢ Reducci√≥n: 15.4%\n",
      "   ‚Ä¢ Promedio palabras/texto: 6.88\n",
      "\n",
      "============================================================\n",
      "üìù EJEMPLOS DE LIMPIEZA\n",
      "============================================================\n",
      "\n",
      "[Ejemplo 1]\n",
      "ORIGINAL:\n",
      "  ¬°¬°¬°Este producto es EXCELENTE!!! Lo recomiendo 100% üòä http://ejemplo.com\n",
      "LIMPIO:\n",
      "  este producto es excelente lo recomiendo\n",
      "\n",
      "[Ejemplo 2]\n",
      "ORIGINAL:\n",
      "  Muy mal servicio, NO lo compren!!! n√∫meros: 12345\n",
      "LIMPIO:\n",
      "  muy mal servicio no lo compren n√∫meros\n",
      "\n",
      "[Ejemplo 3]\n",
      "ORIGINAL:\n",
      "  Buen producto xq cumple con lo esperado tmb es barato\n",
      "LIMPIO:\n",
      "  buen producto porque cumple con lo esperado tambi√©n es barato\n",
      "\n",
      "[Ejemplo 4]\n",
      "ORIGINAL:\n",
      "  TERRIBLE experiencia @usuario #hashtag NO funciona\n",
      "LIMPIO:\n",
      "  terrible experiencia no funciona\n",
      "\n",
      "[Ejemplo 5]\n",
      "ORIGINAL:\n",
      "  Email: test@ejemplo.com y caracteres $$$ especiales !!!\n",
      "LIMPIO:\n",
      "  email y caracteres especiales\n",
      "\n",
      "============================================================\n",
      "üíæ GUARDAR RESULTADOS\n",
      "============================================================\n",
      "‚úì Guardado en: X_train_limpio.pkl\n",
      "‚úì Guardado en: X_train_limpio.csv\n",
      "‚úì Guardado en: X_train_limpio.txt\n",
      "\n",
      "============================================================\n",
      "üìã INSTRUCCIONES DE USO\n",
      "============================================================\n",
      "\n",
      "    Para usar con TU X_train:\n",
      "\n",
      "    # Importar\n",
      "    from limpiar_X_train import limpiar_X_train\n",
      "\n",
      "    # Limpiar (tu X_train puede ser lista, array o Series)\n",
      "    X_train_limpio = limpiar_X_train(X_train)\n",
      "\n",
      "    # O con m√°s control:\n",
      "    from limpiar_X_train import LimpiadorTexto\n",
      "\n",
      "    limpiador = LimpiadorTexto()\n",
      "    X_train_limpio = [limpiador.limpiar_texto_completo(texto) \n",
      "                      for texto in X_train]\n",
      "\n",
      "    # Guardar\n",
      "    import pickle\n",
      "    with open('X_train_limpio.pkl', 'wb') as f:\n",
      "        pickle.dump(X_train_limpio, f)\n",
      "\n",
      "    # Cargar despu√©s\n",
      "    with open('X_train_limpio.pkl', 'rb') as f:\n",
      "        X_train_limpio = pickle.load(f)\n",
      "    \n",
      "\n",
      "‚úÖ Script completado exitosamente!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT PARA LIMPIAR X_TRAIN\n",
    "============================\n",
    "\n",
    "Aplica limpieza de texto a X_train:\n",
    "‚úÖ Convertir a min√∫sculas\n",
    "‚úÖ Eliminar caracteres especiales\n",
    "‚úÖ Eliminar n√∫meros\n",
    "‚úÖ Corregir palabras mal escritas (abreviaturas comunes)\n",
    "‚úÖ Eliminar URLs, emails, menciones\n",
    "‚úÖ Eliminar espacios m√∫ltiples\n",
    "\n",
    "USO DIRECTO CON TU X_TRAIN:\n",
    "----------------------------\n",
    ">>> from limpiar_X_train import limpiar_X_train\n",
    ">>> X_train_limpio = limpiar_X_train(X_train)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LimpiadorTexto:\n",
    "    \"\"\"Limpia texto aplicando todas las transformaciones necesarias\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Diccionario de correcciones de palabras mal escritas\n",
    "        self.correcciones = {\n",
    "            # Abreviaturas comunes\n",
    "            \"q\": \"que\",\n",
    "            \"xq\": \"porque\",\n",
    "            \"pq\": \"porque\",\n",
    "            \"tmb\": \"tambi√©n\",\n",
    "            \"tb\": \"tambi√©n\",\n",
    "            \"x\": \"por\",\n",
    "            \"d\": \"de\",\n",
    "            \"k\": \"que\",\n",
    "            \"bn\": \"bien\",\n",
    "            \"mxo\": \"mucho\",\n",
    "            \"msj\": \"mensaje\",\n",
    "            \"msg\": \"mensaje\",\n",
    "            # Risas\n",
    "            \"jajaja\": \"jaja\",\n",
    "            \"jajajaja\": \"jaja\",\n",
    "            \"jjjjj\": \"jaja\",\n",
    "            \"jeje\": \"jaja\",\n",
    "            \"jiji\": \"jaja\",\n",
    "            \"ajaj\": \"jaja\",\n",
    "            \"jejeje\": \"jaja\",\n",
    "            \"jijiji\": \"jaja\",\n",
    "            # Otras comunes\n",
    "            \"xd\": \"\",\n",
    "            \"lol\": \"\",\n",
    "        }\n",
    "\n",
    "    def a_minusculas(self, texto):\n",
    "        \"\"\"Convierte texto a min√∫sculas\"\"\"\n",
    "        return texto.lower()\n",
    "\n",
    "    def eliminar_urls(self, texto):\n",
    "        \"\"\"Elimina URLs del texto\"\"\"\n",
    "        return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto, flags=re.MULTILINE)\n",
    "\n",
    "    def eliminar_emails(self, texto):\n",
    "        \"\"\"Elimina emails del texto\"\"\"\n",
    "        return re.sub(r\"\\S+@\\S+\", \"\", texto)\n",
    "\n",
    "    def eliminar_menciones_hashtags(self, texto):\n",
    "        \"\"\"Elimina menciones (@usuario) y hashtags (#tema)\"\"\"\n",
    "        texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "        texto = re.sub(r\"#\\w+\", \"\", texto)\n",
    "        return texto\n",
    "\n",
    "    def eliminar_numeros(self, texto):\n",
    "        \"\"\"Elimina n√∫meros del texto\"\"\"\n",
    "        return re.sub(r\"\\d+\", \"\", texto)\n",
    "\n",
    "    def eliminar_caracteres_especiales(self, texto, mantener_espacios=True):\n",
    "        \"\"\"\n",
    "        Elimina caracteres especiales y puntuaci√≥n\n",
    "        Mantiene solo letras (incluyendo √±, acentos) y espacios\n",
    "        \"\"\"\n",
    "        if mantener_espacios:\n",
    "            # Mantener solo letras, acentos y espacios\n",
    "            texto = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú\\s]\", \"\", texto)\n",
    "        else:\n",
    "            texto = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]\", \" \", texto)\n",
    "        return texto\n",
    "\n",
    "    def eliminar_espacios_extra(self, texto):\n",
    "        \"\"\"Elimina espacios m√∫ltiples y limpia el texto\"\"\"\n",
    "        return \" \".join(texto.split()).strip()\n",
    "\n",
    "    def corregir_palabras_comunes(self, texto):\n",
    "        \"\"\"Corrige palabras comunes mal escritas\"\"\"\n",
    "        palabras = texto.split()\n",
    "        palabras_corregidas = [\n",
    "            self.correcciones.get(palabra, palabra) for palabra in palabras\n",
    "        ]\n",
    "        return \" \".join(palabras_corregidas)\n",
    "\n",
    "    def eliminar_palabras_repetidas(self, texto):\n",
    "        \"\"\"Elimina palabras que se repiten consecutivamente\"\"\"\n",
    "        palabras = texto.split()\n",
    "        resultado = []\n",
    "        anterior = None\n",
    "        for palabra in palabras:\n",
    "            if palabra != anterior:\n",
    "                resultado.append(palabra)\n",
    "                anterior = palabra\n",
    "        return \" \".join(resultado)\n",
    "\n",
    "    def limpiar_texto_completo(self, texto):\n",
    "        \"\"\"\n",
    "        Aplica todas las transformaciones de limpieza\n",
    "\n",
    "        Args:\n",
    "            texto: texto a limpiar (string)\n",
    "\n",
    "        Returns:\n",
    "            texto limpio (string)\n",
    "        \"\"\"\n",
    "        if not isinstance(texto, str):\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Convertir a min√∫sculas\n",
    "        texto = self.a_minusculas(texto)\n",
    "\n",
    "        # 2. Eliminar URLs\n",
    "        texto = self.eliminar_urls(texto)\n",
    "\n",
    "        # 3. Eliminar emails\n",
    "        texto = self.eliminar_emails(texto)\n",
    "\n",
    "        # 4. Eliminar menciones y hashtags\n",
    "        texto = self.eliminar_menciones_hashtags(texto)\n",
    "\n",
    "        # 5. Eliminar n√∫meros\n",
    "        texto = self.eliminar_numeros(texto)\n",
    "\n",
    "        # 6. Corregir palabras comunes (antes de eliminar caracteres especiales)\n",
    "        texto = self.corregir_palabras_comunes(texto)\n",
    "\n",
    "        # 7. Eliminar caracteres especiales\n",
    "        texto = self.eliminar_caracteres_especiales(texto)\n",
    "\n",
    "        # 8. Eliminar palabras repetidas\n",
    "        texto = self.eliminar_palabras_repetidas(texto)\n",
    "\n",
    "        # 9. Eliminar espacios extra\n",
    "        texto = self.eliminar_espacios_extra(texto)\n",
    "\n",
    "        return texto\n",
    "\n",
    "\n",
    "def limpiar_X_train(X_train, verbose=True):\n",
    "    \"\"\"\n",
    "    Limpia todos los textos en X_train\n",
    "\n",
    "    Args:\n",
    "        X_train: lista, array, o pandas Series con textos\n",
    "        verbose: mostrar progreso\n",
    "\n",
    "    Returns:\n",
    "        lista con textos limpios\n",
    "    \"\"\"\n",
    "    limpiador = LimpiadorTexto()\n",
    "\n",
    "    X_train_limpio = []\n",
    "    total = len(X_train)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"üßπ LIMPIANDO X_TRAIN\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total de textos: {total}\\n\")\n",
    "\n",
    "    for i, texto in enumerate(X_train):\n",
    "        texto_limpio = limpiador.limpiar_texto_completo(texto)\n",
    "        X_train_limpio.append(texto_limpio)\n",
    "\n",
    "        # Mostrar progreso cada 100 textos\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"‚úì Procesados {i + 1}/{total} textos ({(i + 1) / total * 100:.1f}%)\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ Limpieza completada: {total} textos procesados\")\n",
    "\n",
    "        # Estad√≠sticas\n",
    "        palabras_original = sum(len(str(t).split()) for t in X_train)\n",
    "        palabras_limpio = sum(len(t.split()) for t in X_train_limpio)\n",
    "\n",
    "        print(\"\\nüìä ESTAD√çSTICAS:\")\n",
    "        print(f\"   ‚Ä¢ Palabras originales: {palabras_original}\")\n",
    "        print(f\"   ‚Ä¢ Palabras despu√©s de limpieza: {palabras_limpio}\")\n",
    "        print(f\"   ‚Ä¢ Reducci√≥n: {(1 - palabras_limpio / palabras_original) * 100:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Promedio palabras/texto: {palabras_limpio / total:.2f}\")\n",
    "\n",
    "    return X_train_limpio\n",
    "\n",
    "\n",
    "def mostrar_ejemplos_limpieza(X_train, X_train_limpio, n=5):\n",
    "    \"\"\"\n",
    "    Muestra ejemplos de textos antes y despu√©s de la limpieza\n",
    "\n",
    "    Args:\n",
    "        X_train: textos originales\n",
    "        X_train_limpio: textos limpios\n",
    "        n: n√∫mero de ejemplos a mostrar\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìù EJEMPLOS DE LIMPIEZA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i in range(min(n, len(X_train))):\n",
    "        print(f\"\\n[Ejemplo {i + 1}]\")\n",
    "        print(f\"ORIGINAL:\")\n",
    "        print(f\"  {str(X_train[i])[:100]}{'...' if len(str(X_train[i])) > 100 else ''}\")\n",
    "        print(f\"LIMPIO:\")\n",
    "        print(\n",
    "            f\"  {X_train_limpio[i][:100]}{'...' if len(X_train_limpio[i]) > 100 else ''}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EJEMPLO DE USO\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"üöÄ\" * 30)\n",
    "    print(\"LIMPIEZA DE X_TRAIN\")\n",
    "    print(\"üöÄ\" * 30 + \"\\n\")\n",
    "\n",
    "    # Datos de ejemplo (simula tu X_train)\n",
    "    X_train = [\n",
    "        \"¬°¬°¬°Este producto es EXCELENTE!!! Lo recomiendo 100% üòä http://ejemplo.com\",\n",
    "        \"Muy mal servicio, NO lo compren!!! n√∫meros: 12345\",\n",
    "        \"Buen producto xq cumple con lo esperado tmb es barato\",\n",
    "        \"TERRIBLE experiencia @usuario #hashtag NO funciona\",\n",
    "        \"Email: test@ejemplo.com y caracteres $$$ especiales !!!\",\n",
    "        \"   Texto   con    espacios    m√∫ltiples   repetidos repetidos\",\n",
    "        \"jajajaja muy bueno jeje lo recomiendo jajaja\",\n",
    "        \"Q bueno k funciona bn tmb es r√°pido x eso lo compr√©\",\n",
    "    ]\n",
    "\n",
    "    print(f\"üì¶ Dataset de ejemplo: {len(X_train)} textos\\n\")\n",
    "\n",
    "    # Limpiar X_train\n",
    "    X_train_limpio = limpiar_X_train(X_train, verbose=True)\n",
    "\n",
    "    # Mostrar ejemplos\n",
    "    mostrar_ejemplos_limpieza(X_train, X_train_limpio, n=5)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üíæ GUARDAR RESULTADOS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Opci√≥n 1: Guardar como lista de Python\n",
    "    import pickle\n",
    "\n",
    "    with open(\"X_train_limpio.pkl\", \"wb\") as f:\n",
    "        pickle.dump(X_train_limpio, f)\n",
    "    print(\"‚úì Guardado en: X_train_limpio.pkl\")\n",
    "\n",
    "    # Opci√≥n 2: Guardar como CSV (si tienes pandas)\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.DataFrame({\"texto_original\": X_train, \"texto_limpio\": X_train_limpio})\n",
    "        df.to_csv(\"X_train_limpio.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(\"‚úì Guardado en: X_train_limpio.csv\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  pandas no disponible, CSV no guardado\")\n",
    "\n",
    "    # Opci√≥n 3: Guardar como texto plano\n",
    "    with open(\"X_train_limpio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for texto in X_train_limpio:\n",
    "            f.write(texto + \"\\n\")\n",
    "    print(\"‚úì Guardado en: X_train_limpio.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã INSTRUCCIONES DE USO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    Para usar con TU X_train:\n",
    "    \n",
    "    # Importar\n",
    "    from limpiar_X_train import limpiar_X_train\n",
    "    \n",
    "    # Limpiar (tu X_train puede ser lista, array o Series)\n",
    "    X_train_limpio = limpiar_X_train(X_train)\n",
    "    \n",
    "    # O con m√°s control:\n",
    "    from limpiar_X_train import LimpiadorTexto\n",
    "    \n",
    "    limpiador = LimpiadorTexto()\n",
    "    X_train_limpio = [limpiador.limpiar_texto_completo(texto) \n",
    "                      for texto in X_train]\n",
    "    \n",
    "    # Guardar\n",
    "    import pickle\n",
    "    with open('X_train_limpio.pkl', 'wb') as f:\n",
    "        pickle.dump(X_train_limpio, f)\n",
    "    \n",
    "    # Cargar despu√©s\n",
    "    with open('X_train_limpio.pkl', 'rb') as f:\n",
    "        X_train_limpio = pickle.load(f)\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n‚úÖ Script completado exitosamente!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Representacion del texto\n",
    "\n",
    "Luego de tener los datos limpios, realizar la representaci√≥n de los datos de texto para poder usarse en modelos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TOKENIZACI√ìN + LEMATIZACI√ìN + REPRESENTACI√ìN\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "üì¶ Dataset: 10 textos\n",
      "\n",
      "\n",
      "============================================================\n",
      "OPCI√ìN 1: TF-IDF CON LEMATIZACI√ìN\n",
      "============================================================\n",
      "üîπ PROCESANDO X_TRAIN CON TF-IDF\n",
      "============================================================\n",
      "Total de textos: 10\n",
      "Max features: 50\n",
      "Min df: 1\n",
      "M√©todo: Lematizaci√≥n\n",
      "\n",
      "‚úÖ Procesamiento completado\n",
      "\n",
      "üìä RESULTADOS:\n",
      "   ‚Ä¢ Forma de la matriz: (10, 27)\n",
      "   ‚Ä¢ Vocabulario: 27 palabras\n",
      "   ‚Ä¢ Sparsity: 82.96%\n",
      "\n",
      "üìù Top 10 palabras del vocabulario:\n",
      "    1. learning\n",
      "    2. fascinante\n",
      "    3. machine\n",
      "    4. ser\n",
      "    5. para\n",
      "    6. dato\n",
      "    7. python\n",
      "    8. genial\n",
      "    9. red\n",
      "   10. deep\n",
      "\n",
      "\n",
      "============================================================\n",
      "OPCI√ìN 2: BAG OF WORDS CON STEMMING\n",
      "============================================================\n",
      "üîπ PROCESANDO X_TRAIN CON BAG OF WORDS\n",
      "============================================================\n",
      "Total de textos: 10\n",
      "Max features: 50\n",
      "Min df: 1\n",
      "M√©todo: Stemming\n",
      "\n",
      "‚úÖ Procesamiento completado\n",
      "\n",
      "üìä RESULTADOS:\n",
      "   ‚Ä¢ Forma de la matriz: (10, 27)\n",
      "   ‚Ä¢ Vocabulario: 27 palabras\n",
      "   ‚Ä¢ Valores totales: 46\n",
      "\n",
      "üìù Top 10 palabras del vocabulario:\n",
      "    1. learning\n",
      "    2. es\n",
      "    3. fascin\n",
      "    4. machin\n",
      "    5. dat\n",
      "    6. python\n",
      "    7. par\n",
      "    8. genial\n",
      "    9. red\n",
      "   10. deep\n",
      "\n",
      "============================================================\n",
      "GUARDAR REPRESENTADORES\n",
      "============================================================\n",
      "‚úÖ Representador guardado: representador_tfidf.pkl\n",
      "‚úÖ Representador guardado: representador_bow.pkl\n",
      "\n",
      "============================================================\n",
      "PROCESAR X_TEST\n",
      "============================================================\n",
      "\n",
      "üîπ PROCESANDO X_TEST CON TFIDF\n",
      "============================================================\n",
      "‚úÖ X_test procesado: (2, 27)\n",
      "\n",
      "============================================================\n",
      "üìã C√ìMO USAR CON TU X_TRAIN\n",
      "============================================================\n",
      "\n",
      "    # 1. Importar\n",
      "    from procesar_X_train import procesar_X_train_tfidf, procesar_X_test\n",
      "\n",
      "    # 2. Procesar X_train_limpio\n",
      "    X_train_tfidf, representador = procesar_X_train_tfidf(\n",
      "        X_train_limpio,\n",
      "        max_features=1000,\n",
      "        min_df=2,\n",
      "        usar_lematizacion=True\n",
      "    )\n",
      "\n",
      "    # 3. Procesar X_test_limpio\n",
      "    X_test_tfidf = procesar_X_test(\n",
      "        X_test_limpio,\n",
      "        representador,\n",
      "        metodo='tfidf'\n",
      "    )\n",
      "\n",
      "    # 4. Entrenar modelo\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    modelo = LogisticRegression()\n",
      "    modelo.fit(X_train_tfidf, y_train)\n",
      "\n",
      "    # 5. Evaluar\n",
      "    accuracy = modelo.score(X_test_tfidf, y_test)\n",
      "    print(f\"Accuracy: {accuracy:.2%}\")\n",
      "    \n",
      "\n",
      "‚úÖ Script completado exitosamente!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT PARA TOKENIZACI√ìN + LEMATIZACI√ìN + REPRESENTACI√ìN\n",
    "==========================================================\n",
    "\n",
    "Aplica a X_train_limpio:\n",
    "1. ‚úÖ Tokenizaci√≥n (dividir en palabras)\n",
    "2. ‚úÖ Lematizaci√≥n (reducir a forma base) o Stemming\n",
    "3. ‚úÖ Representaci√≥n num√©rica (Bag of Words o TF-IDF)\n",
    "\n",
    "USO DIRECTO:\n",
    "------------\n",
    ">>> from procesar_X_train import procesar_X_train_tfidf, procesar_X_train_bow\n",
    ">>> X_train_procesado = procesar_X_train_tfidf(X_train_limpio)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 1: TOKENIZADOR\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class Tokenizador:\n",
    "    \"\"\"Divide texto en tokens (palabras)\"\"\"\n",
    "\n",
    "    def tokenizar(self, texto):\n",
    "        \"\"\"Tokeniza un texto en palabras\"\"\"\n",
    "        return texto.split()\n",
    "\n",
    "    def tokenizar_batch(self, textos):\n",
    "        \"\"\"Tokeniza m√∫ltiples textos\"\"\"\n",
    "        return [self.tokenizar(texto) for texto in textos]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 2: STEMMER PARA ESPA√ëOL\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class StemmerEspanol:\n",
    "    \"\"\"Stemmer para espa√±ol - corta palabras a su ra√≠z\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sufijos = [\n",
    "            \"√≠simos\",\n",
    "            \"√≠simas\",\n",
    "            \"amiento\",\n",
    "            \"imientos\",\n",
    "            \"amiento\",\n",
    "            \"aciones\",\n",
    "            \"uci√≥n\",\n",
    "            \"uciones\",\n",
    "            \"adores\",\n",
    "            \"adoras\",\n",
    "            \"√≠simo\",\n",
    "            \"√≠sima\",\n",
    "            \"mente\",\n",
    "            \"ancia\",\n",
    "            \"encia\",\n",
    "            \"aci√≥n\",\n",
    "            \"ici√≥n\",\n",
    "            \"ador\",\n",
    "            \"adora\",\n",
    "            \"ante\",\n",
    "            \"ando\",\n",
    "            \"iendo\",\n",
    "            \"aron\",\n",
    "            \"ieron\",\n",
    "            \"ado\",\n",
    "            \"ido\",\n",
    "            \"able\",\n",
    "            \"ible\",\n",
    "            \"osos\",\n",
    "            \"osas\",\n",
    "            \"ivo\",\n",
    "            \"iva\",\n",
    "            \"dad\",\n",
    "            \"tad\",\n",
    "            \"eza\",\n",
    "            \"miento\",\n",
    "            \"es\",\n",
    "            \"os\",\n",
    "            \"as\",\n",
    "            \"a\",\n",
    "            \"o\",\n",
    "            \"e\",\n",
    "        ]\n",
    "        self.min_length = 3\n",
    "\n",
    "    def stem(self, palabra):\n",
    "        \"\"\"Reduce una palabra a su ra√≠z\"\"\"\n",
    "        if len(palabra) <= self.min_length:\n",
    "            return palabra\n",
    "\n",
    "        for sufijo in self.sufijos:\n",
    "            if (\n",
    "                palabra.endswith(sufijo)\n",
    "                and len(palabra) - len(sufijo) >= self.min_length\n",
    "            ):\n",
    "                return palabra[: -len(sufijo)]\n",
    "\n",
    "        return palabra\n",
    "\n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Aplica stemming a lista de tokens\"\"\"\n",
    "        return [self.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 3: LEMATIZADOR PARA ESPA√ëOL\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class LematizadorEspanol:\n",
    "    \"\"\"Lematizador para espa√±ol - convierte a forma can√≥nica\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.diccionario_lemas = {\n",
    "            # Verbos ser/estar\n",
    "            \"soy\": \"ser\",\n",
    "            \"eres\": \"ser\",\n",
    "            \"es\": \"ser\",\n",
    "            \"somos\": \"ser\",\n",
    "            \"son\": \"ser\",\n",
    "            \"fui\": \"ser\",\n",
    "            \"fue\": \"ser\",\n",
    "            \"fueron\": \"ser\",\n",
    "            \"siendo\": \"ser\",\n",
    "            \"sido\": \"ser\",\n",
    "            \"estoy\": \"estar\",\n",
    "            \"est√°s\": \"estar\",\n",
    "            \"est√°\": \"estar\",\n",
    "            \"estamos\": \"estar\",\n",
    "            \"est√°n\": \"estar\",\n",
    "            \"estuve\": \"estar\",\n",
    "            \"estuvo\": \"estar\",\n",
    "            \"estado\": \"estar\",\n",
    "            # Verbo tener\n",
    "            \"tengo\": \"tener\",\n",
    "            \"tienes\": \"tener\",\n",
    "            \"tiene\": \"tener\",\n",
    "            \"tenemos\": \"tener\",\n",
    "            \"tienen\": \"tener\",\n",
    "            \"tuve\": \"tener\",\n",
    "            \"tuvo\": \"tener\",\n",
    "            \"tenido\": \"tener\",\n",
    "            # Verbo hacer\n",
    "            \"hago\": \"hacer\",\n",
    "            \"haces\": \"hacer\",\n",
    "            \"hace\": \"hacer\",\n",
    "            \"hacemos\": \"hacer\",\n",
    "            \"hacen\": \"hacer\",\n",
    "            \"hice\": \"hacer\",\n",
    "            \"hizo\": \"hacer\",\n",
    "            \"hecho\": \"hacer\",\n",
    "            # Verbo ir\n",
    "            \"voy\": \"ir\",\n",
    "            \"vas\": \"ir\",\n",
    "            \"va\": \"ir\",\n",
    "            \"vamos\": \"ir\",\n",
    "            \"van\": \"ir\",\n",
    "            \"ido\": \"ir\",\n",
    "            # Plurales comunes\n",
    "            \"datos\": \"dato\",\n",
    "            \"textos\": \"texto\",\n",
    "            \"palabras\": \"palabra\",\n",
    "            \"modelos\": \"modelo\",\n",
    "            \"redes\": \"red\",\n",
    "            \"√°rboles\": \"√°rbol\",\n",
    "        }\n",
    "\n",
    "    def lematizar(self, palabra):\n",
    "        \"\"\"Obtiene el lema de una palabra\"\"\"\n",
    "        if palabra in self.diccionario_lemas:\n",
    "            return self.diccionario_lemas[palabra]\n",
    "\n",
    "        # Reglas simples para plurales\n",
    "        if palabra.endswith(\"es\") and len(palabra) > 4:\n",
    "            return palabra[:-2]\n",
    "        elif palabra.endswith(\"s\") and len(palabra) > 3:\n",
    "            return palabra[:-1]\n",
    "\n",
    "        return palabra\n",
    "\n",
    "    def lematizar_tokens(self, tokens):\n",
    "        \"\"\"Aplica lematizaci√≥n a lista de tokens\"\"\"\n",
    "        return [self.lematizar(token) for token in tokens]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 4: REPRESENTADOR (BAG OF WORDS Y TF-IDF)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RepresentadorTexto:\n",
    "    \"\"\"\n",
    "    Convierte textos en vectores num√©ricos despu√©s de tokenizar y lematizar\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, usar_lematizacion=True, usar_stemming=False, max_features=1000, min_df=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            usar_lematizacion: si usar lematizaci√≥n\n",
    "            usar_stemming: si usar stemming\n",
    "            max_features: n√∫mero m√°ximo de palabras en vocabulario\n",
    "            min_df: frecuencia m√≠nima de palabra\n",
    "        \"\"\"\n",
    "        self.usar_lematizacion = usar_lematizacion\n",
    "        self.usar_stemming = usar_stemming\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "\n",
    "        self.tokenizador = Tokenizador()\n",
    "        self.lematizador = LematizadorEspanol() if usar_lematizacion else None\n",
    "        self.stemmer = StemmerEspanol() if usar_stemming else None\n",
    "\n",
    "        self.vocabulario = {}\n",
    "        self.feature_names = []\n",
    "        self.idf_dict = {}\n",
    "\n",
    "    def _procesar_texto(self, texto):\n",
    "        \"\"\"Tokeniza y aplica lematizaci√≥n o stemming\"\"\"\n",
    "        # Tokenizar\n",
    "        tokens = self.tokenizador.tokenizar(texto)\n",
    "\n",
    "        # Lematizar o hacer stemming\n",
    "        if self.usar_lematizacion:\n",
    "            tokens = self.lematizador.lematizar_tokens(tokens)\n",
    "        elif self.usar_stemming:\n",
    "            tokens = self.stemmer.stem_tokens(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def fit(self, textos):\n",
    "        \"\"\"Construye vocabulario a partir de textos\"\"\"\n",
    "        # Procesar todos los textos\n",
    "        todos_tokens = []\n",
    "        for texto in textos:\n",
    "            tokens = self._procesar_texto(texto)\n",
    "            todos_tokens.append(tokens)\n",
    "\n",
    "        # Contar frecuencia de documentos\n",
    "        doc_freq = Counter()\n",
    "        for tokens in todos_tokens:\n",
    "            tokens_unicos = set(tokens)\n",
    "            doc_freq.update(tokens_unicos)\n",
    "\n",
    "        # Filtrar por frecuencia m√≠nima\n",
    "        tokens_validos = [\n",
    "            token for token, freq in doc_freq.items() if freq >= self.min_df\n",
    "        ]\n",
    "\n",
    "        # Limitar features\n",
    "        if self.max_features and len(tokens_validos) > self.max_features:\n",
    "            tokens_ordenados = sorted(\n",
    "                doc_freq.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            tokens_validos = [t[0] for t in tokens_ordenados[: self.max_features]]\n",
    "\n",
    "        # Crear vocabulario\n",
    "        self.vocabulario = {token: idx for idx, token in enumerate(tokens_validos)}\n",
    "        self.feature_names = tokens_validos\n",
    "\n",
    "        # Calcular IDF\n",
    "        n_docs = len(textos)\n",
    "        for token in tokens_validos:\n",
    "            df = doc_freq[token]\n",
    "            self.idf_dict[token] = np.log(n_docs / df) + 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform_bow(self, textos):\n",
    "        \"\"\"Transforma textos a Bag of Words\"\"\"\n",
    "        X = np.zeros((len(textos), len(self.vocabulario)))\n",
    "\n",
    "        for i, texto in enumerate(textos):\n",
    "            tokens = self._procesar_texto(texto)\n",
    "            token_counts = Counter(tokens)\n",
    "\n",
    "            for token, count in token_counts.items():\n",
    "                if token in self.vocabulario:\n",
    "                    idx = self.vocabulario[token]\n",
    "                    X[i, idx] = count\n",
    "\n",
    "        return X\n",
    "\n",
    "    def transform_tfidf(self, textos):\n",
    "        \"\"\"Transforma textos a TF-IDF\"\"\"\n",
    "        X = np.zeros((len(textos), len(self.vocabulario)))\n",
    "\n",
    "        for i, texto in enumerate(textos):\n",
    "            tokens = self._procesar_texto(texto)\n",
    "            token_counts = Counter(tokens)\n",
    "\n",
    "            # Calcular TF-IDF\n",
    "            for token, tf in token_counts.items():\n",
    "                if token in self.vocabulario:\n",
    "                    idx = self.vocabulario[token]\n",
    "                    idf = self.idf_dict[token]\n",
    "                    X[i, idx] = tf * idf\n",
    "\n",
    "        # Normalizaci√≥n L2\n",
    "        normas = np.sqrt(np.sum(X**2, axis=1, keepdims=True))\n",
    "        normas[normas == 0] = 1\n",
    "        X = X / normas\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform_bow(self, textos):\n",
    "        \"\"\"Ajusta y transforma a BoW\"\"\"\n",
    "        self.fit(textos)\n",
    "        return self.transform_bow(textos)\n",
    "\n",
    "    def fit_transform_tfidf(self, textos):\n",
    "        \"\"\"Ajusta y transforma a TF-IDF\"\"\"\n",
    "        self.fit(textos)\n",
    "        return self.transform_tfidf(textos)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCIONES DE USO DIRECTO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def procesar_X_train_tfidf(\n",
    "    X_train_limpio, max_features=1000, min_df=2, usar_lematizacion=True, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa X_train_limpio y genera representaci√≥n TF-IDF\n",
    "\n",
    "    Args:\n",
    "        X_train_limpio: lista de textos ya limpios\n",
    "        max_features: n√∫mero m√°ximo de palabras\n",
    "        min_df: frecuencia m√≠nima de palabra\n",
    "        usar_lematizacion: True para lematizaci√≥n, False para stemming\n",
    "        verbose: mostrar progreso\n",
    "\n",
    "    Returns:\n",
    "        X_tfidf: matriz TF-IDF (numpy array)\n",
    "        representador: objeto entrenado (para transformar X_test despu√©s)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"üîπ PROCESANDO X_TRAIN CON TF-IDF\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total de textos: {len(X_train_limpio)}\")\n",
    "        print(f\"Max features: {max_features}\")\n",
    "        print(f\"Min df: {min_df}\")\n",
    "        print(f\"M√©todo: {'Lematizaci√≥n' if usar_lematizacion else 'Stemming'}\\n\")\n",
    "\n",
    "    # Crear representador\n",
    "    representador = RepresentadorTexto(\n",
    "        usar_lematizacion=usar_lematizacion,\n",
    "        usar_stemming=not usar_lematizacion,\n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "    )\n",
    "\n",
    "    # Procesar\n",
    "    X_tfidf = representador.fit_transform_tfidf(X_train_limpio)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Procesamiento completado\")\n",
    "        print(f\"\\nüìä RESULTADOS:\")\n",
    "        print(f\"   ‚Ä¢ Forma de la matriz: {X_tfidf.shape}\")\n",
    "        print(f\"   ‚Ä¢ Vocabulario: {len(representador.vocabulario)} palabras\")\n",
    "        print(f\"   ‚Ä¢ Sparsity: {(X_tfidf == 0).sum() / X_tfidf.size * 100:.2f}%\")\n",
    "\n",
    "        print(f\"\\nüìù Top 10 palabras del vocabulario:\")\n",
    "        for i, palabra in enumerate(representador.feature_names[:10], 1):\n",
    "            print(f\"   {i:2d}. {palabra}\")\n",
    "\n",
    "    return X_tfidf, representador\n",
    "\n",
    "\n",
    "def procesar_X_train_bow(\n",
    "    X_train_limpio, max_features=1000, min_df=2, usar_lematizacion=True, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa X_train_limpio y genera representaci√≥n Bag of Words\n",
    "\n",
    "    Args:\n",
    "        X_train_limpio: lista de textos ya limpios\n",
    "        max_features: n√∫mero m√°ximo de palabras\n",
    "        min_df: frecuencia m√≠nima de palabra\n",
    "        usar_lematizacion: True para lematizaci√≥n, False para stemming\n",
    "        verbose: mostrar progreso\n",
    "\n",
    "    Returns:\n",
    "        X_bow: matriz Bag of Words (numpy array)\n",
    "        representador: objeto entrenado (para transformar X_test despu√©s)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"üîπ PROCESANDO X_TRAIN CON BAG OF WORDS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total de textos: {len(X_train_limpio)}\")\n",
    "        print(f\"Max features: {max_features}\")\n",
    "        print(f\"Min df: {min_df}\")\n",
    "        print(f\"M√©todo: {'Lematizaci√≥n' if usar_lematizacion else 'Stemming'}\\n\")\n",
    "\n",
    "    # Crear representador\n",
    "    representador = RepresentadorTexto(\n",
    "        usar_lematizacion=usar_lematizacion,\n",
    "        usar_stemming=not usar_lematizacion,\n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "    )\n",
    "\n",
    "    # Procesar\n",
    "    X_bow = representador.fit_transform_bow(X_train_limpio)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Procesamiento completado\")\n",
    "        print(f\"\\nüìä RESULTADOS:\")\n",
    "        print(f\"   ‚Ä¢ Forma de la matriz: {X_bow.shape}\")\n",
    "        print(f\"   ‚Ä¢ Vocabulario: {len(representador.vocabulario)} palabras\")\n",
    "        print(f\"   ‚Ä¢ Valores totales: {X_bow.sum():.0f}\")\n",
    "\n",
    "        print(f\"\\nüìù Top 10 palabras del vocabulario:\")\n",
    "        for i, palabra in enumerate(representador.feature_names[:10], 1):\n",
    "            print(f\"   {i:2d}. {palabra}\")\n",
    "\n",
    "    return X_bow, representador\n",
    "\n",
    "\n",
    "def procesar_X_test(X_test_limpio, representador, metodo=\"tfidf\", verbose=True):\n",
    "    \"\"\"\n",
    "    Procesa X_test usando el mismo representador de X_train\n",
    "\n",
    "    Args:\n",
    "        X_test_limpio: lista de textos ya limpios\n",
    "        representador: objeto RepresentadorTexto ya entrenado\n",
    "        metodo: 'tfidf' o 'bow'\n",
    "        verbose: mostrar progreso\n",
    "\n",
    "    Returns:\n",
    "        X_test_procesado: matriz procesada\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nüîπ PROCESANDO X_TEST CON {metodo.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    if metodo == \"tfidf\":\n",
    "        X_test_procesado = representador.transform_tfidf(X_test_limpio)\n",
    "    else:\n",
    "        X_test_procesado = representador.transform_bow(X_test_limpio)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"‚úÖ X_test procesado: {X_test_procesado.shape}\")\n",
    "\n",
    "    return X_test_procesado\n",
    "\n",
    "\n",
    "def guardar_representador(representador, filename=\"representador.pkl\"):\n",
    "    \"\"\"Guarda el representador entrenado\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(representador, f)\n",
    "    print(f\"‚úÖ Representador guardado: {filename}\")\n",
    "\n",
    "\n",
    "def cargar_representador(filename=\"representador.pkl\"):\n",
    "    \"\"\"Carga un representador guardado\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        representador = pickle.load(f)\n",
    "    print(f\"‚úÖ Representador cargado: {filename}\")\n",
    "    return representador\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EJEMPLO DE USO\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"üöÄ\" * 30)\n",
    "    print(\"TOKENIZACI√ìN + LEMATIZACI√ìN + REPRESENTACI√ìN\")\n",
    "    print(\"üöÄ\" * 30 + \"\\n\")\n",
    "\n",
    "    # Simular X_train_limpio (textos ya limpios)\n",
    "    X_train_limpio = [\n",
    "        \"machine learning es fascinante\",\n",
    "        \"python es genial para datos\",\n",
    "        \"deep learning utiliza redes neuronales\",\n",
    "        \"procesamiento de lenguaje natural\",\n",
    "        \"algoritmos de clasificaci√≥n y regresi√≥n\",\n",
    "        \"modelos de machine learning\",\n",
    "        \"entrenamiento de redes neuronales\",\n",
    "        \"an√°lisis de datos con python\",\n",
    "        \"inteligencia artificial y machine learning\",\n",
    "        \"procesamiento de texto con python\",\n",
    "    ]\n",
    "\n",
    "    print(f\"üì¶ Dataset: {len(X_train_limpio)} textos\\n\")\n",
    "\n",
    "    # OPCI√ìN 1: TF-IDF con Lematizaci√≥n\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPCI√ìN 1: TF-IDF CON LEMATIZACI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    X_tfidf, representador_tfidf = procesar_X_train_tfidf(\n",
    "        X_train_limpio, max_features=50, min_df=1, usar_lematizacion=True, verbose=True\n",
    "    )\n",
    "\n",
    "    # OPCI√ìN 2: Bag of Words con Stemming\n",
    "    print(\"\\n\\n\" + \"=\" * 60)\n",
    "    print(\"OPCI√ìN 2: BAG OF WORDS CON STEMMING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    X_bow, representador_bow = procesar_X_train_bow(\n",
    "        X_train_limpio,\n",
    "        max_features=50,\n",
    "        min_df=1,\n",
    "        usar_lematizacion=False,  # Usar stemming\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Guardar representadores\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GUARDAR REPRESENTADORES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    guardar_representador(representador_tfidf, \"representador_tfidf.pkl\")\n",
    "    guardar_representador(representador_bow, \"representador_bow.pkl\")\n",
    "\n",
    "    # Ejemplo con X_test\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESAR X_TEST\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    X_test_limpio = [\"machine learning con python\", \"redes neuronales profundas\"]\n",
    "\n",
    "    X_test_tfidf = procesar_X_test(\n",
    "        X_test_limpio, representador_tfidf, metodo=\"tfidf\", verbose=True\n",
    "    )\n",
    "\n",
    "    # Instrucciones\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã C√ìMO USAR CON TU X_TRAIN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    # 1. Importar\n",
    "    from procesar_X_train import procesar_X_train_tfidf, procesar_X_test\n",
    "    \n",
    "    # 2. Procesar X_train_limpio\n",
    "    X_train_tfidf, representador = procesar_X_train_tfidf(\n",
    "        X_train_limpio,\n",
    "        max_features=1000,\n",
    "        min_df=2,\n",
    "        usar_lematizacion=True\n",
    "    )\n",
    "    \n",
    "    # 3. Procesar X_test_limpio\n",
    "    X_test_tfidf = procesar_X_test(\n",
    "        X_test_limpio,\n",
    "        representador,\n",
    "        metodo='tfidf'\n",
    "    )\n",
    "    \n",
    "    # 4. Entrenar modelo\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    modelo = LogisticRegression()\n",
    "    modelo.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # 5. Evaluar\n",
    "    accuracy = modelo.score(X_test_tfidf, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n‚úÖ Script completado exitosamente!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Yvvopa2mVox"
   },
   "source": [
    "## 3) Entrenar un modelo de machine learning de clasificaci√≥n\n",
    "\n",
    "Utilizar un modelo de clasificaci√≥n para entrenar y evaluar el modelo con los datos preparados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1742314553868,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "Xm_vBOhymWjZ",
    "outputId": "1f7e8842-fdb5-4580-998a-797d8c69e20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "ENTRENAR MODELO DE CLASIFICACI√ìN - EJEMPLO\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "üì¶ Datos de ejemplo generados\n",
      "   Forma: (200, 50)\n",
      "   Clases: [0 1]\n",
      "\n",
      "ü§ñ ENTRENANDO MODELO DE CLASIFICACI√ìN\n",
      "============================================================\n",
      "Tipo de modelo: logistic\n",
      "Total de muestras: 200\n",
      "Caracter√≠sticas: 50\n",
      "Clases: [0 1]\n",
      "\n",
      "Train: 160 muestras\n",
      "Validaci√≥n: 40 muestras\n",
      "\n",
      "Entrenando logistic...\n",
      "‚úÖ Modelo entrenado\n",
      "\n",
      "üìä RESULTADOS EN VALIDACI√ìN:\n",
      "   Accuracy: 0.7250\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73        19\n",
      "           1       0.78      0.67      0.72        21\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.73      0.73      0.72        40\n",
      "weighted avg       0.73      0.72      0.72        40\n",
      "\n",
      "\n",
      "üîÑ Re-entrenando con todos los datos...\n",
      "‚úÖ Modelo final entrenado con todos los datos\n",
      "\n",
      "============================================================\n",
      "üíæ GUARDANDO MODELO\n",
      "============================================================\n",
      "‚úÖ Modelo guardado: mi_modelo.pkl\n",
      "\n",
      "============================================================\n",
      "üîÆ CARGAR Y PREDECIR\n",
      "============================================================\n",
      "‚úÖ Modelo cargado: mi_modelo.pkl\n",
      "\n",
      "‚úÖ Predicciones: [1 1 0 0 1]\n",
      "   Reales:       [1 1 0 0 1]\n",
      "\n",
      "============================================================\n",
      "üìã C√ìMO USAR CON TUS DATOS\n",
      "============================================================\n",
      "\n",
      "    # ========================================\n",
      "    # PASO 1: Entrenar con tus datos\n",
      "    # ========================================\n",
      "    from entrenar_modelo import entrenar_modelo, guardar_modelo\n",
      "\n",
      "    # Tu X_train ya procesado (matriz TF-IDF o BoW)\n",
      "    # Tu y_train (etiquetas)\n",
      "\n",
      "    modelo = entrenar_modelo(\n",
      "        X_train_procesado,\n",
      "        y_train,\n",
      "        modelo_tipo='logistic',  # o 'random_forest', 'svm', etc.\n",
      "        test_size=0.2\n",
      "    )\n",
      "\n",
      "    guardar_modelo(modelo, 'mi_modelo_final.pkl')\n",
      "\n",
      "\n",
      "    # ========================================\n",
      "    # PASO 2: Usar en producci√≥n\n",
      "    # ========================================\n",
      "    from entrenar_modelo import cargar_modelo, predecir\n",
      "\n",
      "    # Cargar modelo\n",
      "    modelo = cargar_modelo('mi_modelo_final.pkl')\n",
      "\n",
      "    # Predecir (X_nuevos debe estar procesado igual que X_train)\n",
      "    predicciones = predecir(modelo, X_nuevos_procesados)\n",
      "\n",
      "    print(predicciones)\n",
      "    \n",
      "\n",
      "‚úÖ Ejemplo completado!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENTRENAR MODELO DE CLASIFICACI√ìN\n",
    "=================================\n",
    "\n",
    "Script simple para entrenar un modelo de clasificaci√≥n\n",
    "con los datos ya preparados\n",
    "\n",
    "USO:\n",
    "----\n",
    ">>> from entrenar_modelo import entrenar_modelo\n",
    ">>> modelo = entrenar_modelo(X_train_procesado, y_train)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def entrenar_modelo(\n",
    "    X_train, y_train, modelo_tipo=\"logistic\", test_size=0.2, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de clasificaci√≥n\n",
    "\n",
    "    Args:\n",
    "        X_train: matriz de caracter√≠sticas (ya procesada con TF-IDF o BoW)\n",
    "        y_train: etiquetas\n",
    "        modelo_tipo: tipo de modelo a entrenar\n",
    "            - 'logistic': Logistic Regression (RECOMENDADO)\n",
    "            - 'random_forest': Random Forest\n",
    "            - 'svm': Support Vector Machine\n",
    "            - 'decision_tree': Decision Tree\n",
    "            - 'knn': K-Nearest Neighbors\n",
    "            - 'gradient_boosting': Gradient Boosting\n",
    "        test_size: proporci√≥n para validaci√≥n (0.2 = 20%)\n",
    "        verbose: mostrar informaci√≥n\n",
    "\n",
    "    Returns:\n",
    "        modelo entrenado\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ü§ñ ENTRENANDO MODELO DE CLASIFICACI√ìN\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Tipo de modelo: {modelo_tipo}\")\n",
    "        print(f\"Total de muestras: {len(X_train)}\")\n",
    "        print(f\"Caracter√≠sticas: {X_train.shape[1]}\")\n",
    "        print(f\"Clases: {np.unique(y_train)}\\n\")\n",
    "\n",
    "    # Dividir en train/validaci√≥n\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=test_size, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Train: {X_train_split.shape[0]} muestras\")\n",
    "        print(f\"Validaci√≥n: {X_val.shape[0]} muestras\\n\")\n",
    "\n",
    "    # Seleccionar modelo\n",
    "    if modelo_tipo == \"logistic\":\n",
    "        modelo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    elif modelo_tipo == \"random_forest\":\n",
    "        modelo = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    elif modelo_tipo == \"svm\":\n",
    "        modelo = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "\n",
    "    elif modelo_tipo == \"decision_tree\":\n",
    "        modelo = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "    elif modelo_tipo == \"knn\":\n",
    "        modelo = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    elif modelo_tipo == \"gradient_boosting\":\n",
    "        modelo = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo '{modelo_tipo}' no soportado\")\n",
    "\n",
    "    # Entrenar\n",
    "    if verbose:\n",
    "        print(f\"Entrenando {modelo_tipo}...\")\n",
    "\n",
    "    modelo.fit(X_train_split, y_train_split)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"‚úÖ Modelo entrenado\\n\")\n",
    "\n",
    "    # Evaluar en validaci√≥n\n",
    "    y_val_pred = modelo.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"üìä RESULTADOS EN VALIDACI√ìN:\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\\n\")\n",
    "        print(classification_report(y_val, y_val_pred, zero_division=0))\n",
    "\n",
    "    # Re-entrenar con todos los datos\n",
    "    if verbose:\n",
    "        print(\"\\nüîÑ Re-entrenando con todos los datos...\")\n",
    "\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"‚úÖ Modelo final entrenado con todos los datos\\n\")\n",
    "\n",
    "    return modelo\n",
    "\n",
    "\n",
    "def guardar_modelo(modelo, filename=\"modelo.pkl\"):\n",
    "    \"\"\"\n",
    "    Guarda el modelo entrenado\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo entrenado\n",
    "        filename: nombre del archivo\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    print(f\"‚úÖ Modelo guardado: {filename}\")\n",
    "\n",
    "\n",
    "def cargar_modelo(filename=\"modelo.pkl\"):\n",
    "    \"\"\"\n",
    "    Carga un modelo guardado\n",
    "\n",
    "    Args:\n",
    "        filename: nombre del archivo\n",
    "\n",
    "    Returns:\n",
    "        modelo cargado\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        modelo = pickle.load(f)\n",
    "    print(f\"‚úÖ Modelo cargado: {filename}\")\n",
    "    return modelo\n",
    "\n",
    "\n",
    "def predecir(modelo, X_nuevos):\n",
    "    \"\"\"\n",
    "    Predice con el modelo entrenado\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo entrenado\n",
    "        X_nuevos: datos nuevos (ya procesados)\n",
    "\n",
    "    Returns:\n",
    "        predicciones\n",
    "    \"\"\"\n",
    "    return modelo.predict(X_nuevos)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EJEMPLO DE USO\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"üöÄ\" * 30)\n",
    "    print(\"ENTRENAR MODELO DE CLASIFICACI√ìN - EJEMPLO\")\n",
    "    print(\"üöÄ\" * 30 + \"\\n\")\n",
    "\n",
    "    # Generar datos de ejemplo\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=200, n_features=50, n_informative=30, n_classes=2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"üì¶ Datos de ejemplo generados\")\n",
    "    print(f\"   Forma: {X.shape}\")\n",
    "    print(f\"   Clases: {np.unique(y)}\\n\")\n",
    "\n",
    "    # Entrenar modelo\n",
    "    modelo = entrenar_modelo(X, y, modelo_tipo=\"logistic\", test_size=0.2, verbose=True)\n",
    "\n",
    "    # Guardar modelo\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üíæ GUARDANDO MODELO\")\n",
    "    print(\"=\" * 60)\n",
    "    guardar_modelo(modelo, \"mi_modelo.pkl\")\n",
    "\n",
    "    # Cargar y usar\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîÆ CARGAR Y PREDECIR\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    modelo_cargado = cargar_modelo(\"mi_modelo.pkl\")\n",
    "\n",
    "    # Predecir con nuevos datos\n",
    "    X_nuevos = X[:5]\n",
    "    predicciones = predecir(modelo_cargado, X_nuevos)\n",
    "\n",
    "    print(f\"\\n‚úÖ Predicciones: {predicciones}\")\n",
    "    print(f\"   Reales:       {y[:5]}\")\n",
    "\n",
    "    # Instrucciones de uso\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã C√ìMO USAR CON TUS DATOS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    # ========================================\n",
    "    # PASO 1: Entrenar con tus datos\n",
    "    # ========================================\n",
    "    from entrenar_modelo import entrenar_modelo, guardar_modelo\n",
    "    \n",
    "    # Tu X_train ya procesado (matriz TF-IDF o BoW)\n",
    "    # Tu y_train (etiquetas)\n",
    "    \n",
    "    modelo = entrenar_modelo(\n",
    "        X_train_procesado,\n",
    "        y_train,\n",
    "        modelo_tipo='logistic',  # o 'random_forest', 'svm', etc.\n",
    "        test_size=0.2\n",
    "    )\n",
    "    \n",
    "    guardar_modelo(modelo, 'mi_modelo_final.pkl')\n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 2: Usar en producci√≥n\n",
    "    # ========================================\n",
    "    from entrenar_modelo import cargar_modelo, predecir\n",
    "    \n",
    "    # Cargar modelo\n",
    "    modelo = cargar_modelo('mi_modelo_final.pkl')\n",
    "    \n",
    "    # Predecir (X_nuevos debe estar procesado igual que X_train)\n",
    "    predicciones = predecir(modelo, X_nuevos_procesados)\n",
    "    \n",
    "    print(predicciones)\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n‚úÖ Ejemplo completado!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt0Mxr1YmZO0"
   },
   "source": [
    "### Evaluar el modelo con los datos de prueba\n",
    "\n",
    "Usar el modelo para predecir en `X_test` y evaluar con `y_test`\n",
    "\n",
    "**Nota:** Recuerde que `X_test` debe pasar por los mismos procesos de limpieza y representaci√≥n que `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶\n",
      "DATOS DE EJEMPLO - REEMPLAZA CON TUS DATOS\n",
      "üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶\n",
      "\n",
      "X_train: 20 muestras\n",
      "X_test:  5 muestras\n",
      "Clases:  [0 1]\n",
      "\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "PIPELINE COMPLETO - CLASIFICACI√ìN DE TEXTO\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "============================================================\n",
      "PASO 1: LIMPIEZA\n",
      "============================================================\n",
      "Limpiando X_train...\n",
      "‚úì 20 textos limpios\n",
      "Limpiando X_test...\n",
      "‚úì 5 textos limpios\n",
      "\n",
      "Ejemplo de limpieza:\n",
      "  Original: Este producto es excelente me encant√≥...\n",
      "  Limpio:   este producto es excelente me encant√≥\n",
      "\n",
      "============================================================\n",
      "PASO 2: TOKENIZACI√ìN + LEMATIZACI√ìN + TF-IDF\n",
      "============================================================\n",
      "Procesando X_train...\n",
      "‚úì Matriz TF-IDF: (20, 61)\n",
      "‚úì Vocabulario: 61 palabras\n",
      "\n",
      "Procesando X_test con el MISMO representador...\n",
      "‚úì Matriz TF-IDF: (5, 61)\n",
      "\n",
      "============================================================\n",
      "PASO 3: ENTRENAMIENTO\n",
      "============================================================\n",
      "ü§ñ ENTRENANDO MODELO\n",
      "============================================================\n",
      "Tipo: logistic\n",
      "Muestras: 20\n",
      "‚úÖ Modelo entrenado\n",
      "\n",
      "============================================================\n",
      "PASO 4: EVALUACI√ìN CON X_TEST\n",
      "============================================================\n",
      "üîç EVALUANDO MODELO\n",
      "============================================================\n",
      "\n",
      "‚úÖ ACCURACY: 1.0000 (100.00%)\n",
      "\n",
      "üìä MATRIZ DE CONFUSI√ìN:\n",
      "[[2 0]\n",
      " [0 3]]\n",
      "\n",
      "üìà REPORTE DETALLADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "============================================================\n",
      "üîç PREDICCIONES vs REALES\n",
      "============================================================\n",
      "\n",
      "1. ‚úÖ Real: 1 | Predicho: 1\n",
      "   Texto: Producto excelente muy recomendable...\n",
      "\n",
      "2. ‚úÖ Real: 0 | Predicho: 0\n",
      "   Texto: No lo compren muy malo...\n",
      "\n",
      "3. ‚úÖ Real: 1 | Predicho: 1\n",
      "   Texto: Buena opci√≥n cumple expectativas...\n",
      "\n",
      "4. ‚úÖ Real: 0 | Predicho: 0\n",
      "   Texto: P√©simo no sirve para nada...\n",
      "\n",
      "5. ‚úÖ Real: 1 | Predicho: 1\n",
      "   Texto: Genial estoy muy contento...\n",
      "\n",
      "============================================================\n",
      "üíæ GUARDANDO MODELO Y REPRESENTADOR\n",
      "============================================================\n",
      "‚úì Modelo guardado: modelo_final.pkl\n",
      "‚úì Representador guardado: representador_final.pkl\n",
      "\n",
      "============================================================\n",
      "üìä RESUMEN\n",
      "============================================================\n",
      "\n",
      "    ‚úÖ X_train: 20 muestras\n",
      "    ‚úÖ X_test:  5 muestras\n",
      "    ‚úÖ Vocabulario: 61 palabras\n",
      "    ‚úÖ Accuracy: 100.00%\n",
      "\n",
      "    üìÇ Archivos generados:\n",
      "       ‚Ä¢ modelo_final.pkl\n",
      "       ‚Ä¢ representador_final.pkl\n",
      "    \n",
      "\n",
      "============================================================\n",
      "üîÆ PREDICCI√ìN CON TEXTOS NUEVOS\n",
      "============================================================\n",
      "\n",
      "1. Texto: Este producto es incre√≠ble lo recomiendo\n",
      "   Predicci√≥n: POSITIVO ‚úÖ\n",
      "   Confianza: 58.40%\n",
      "\n",
      "2. Texto: Horrible experiencia no funciona\n",
      "   Predicci√≥n: NEGATIVO ‚ùå\n",
      "   Confianza: 69.25%\n",
      "\n",
      "3. Texto: Producto correcto cumple funci√≥n\n",
      "   Predicci√≥n: POSITIVO ‚úÖ\n",
      "   Confianza: 64.30%\n",
      "\n",
      "============================================================\n",
      "‚úÖ ¬°COMPLETADO!\n",
      "============================================================\n",
      "\n",
      "    Para usar con TUS datos:\n",
      "\n",
      "    1. Reemplaza las l√≠neas 324-347 con tus datos reales:\n",
      "\n",
      "       import pandas as pd\n",
      "\n",
      "       df = pd.read_csv('tus_datos.csv')\n",
      "       X_train_crudo = df['columna_texto'].tolist()\n",
      "       y_train = df['columna_etiquetas'].tolist()\n",
      "       # etc...\n",
      "\n",
      "    2. Ejecuta el script:\n",
      "\n",
      "       python script_completo_todo_en_uno.py\n",
      "\n",
      "    3. Usa los archivos generados:\n",
      "\n",
      "       ‚Ä¢ modelo_final.pkl\n",
      "       ‚Ä¢ representador_final.pkl\n",
      "\n",
      "    ¬°Listo! üéâ\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT TODO-EN-UNO COMPLETO\n",
    "============================\n",
    "\n",
    "Este script contiene TODO lo necesario en un solo archivo:\n",
    "1. Limpieza\n",
    "2. Tokenizaci√≥n + Lematizaci√≥n\n",
    "3. Representaci√≥n (TF-IDF)\n",
    "4. Entrenamiento\n",
    "5. Evaluaci√≥n\n",
    "\n",
    "NO necesita importar otros scripts.\n",
    "Solo reemplaza tus datos en el PASO 1.\n",
    "\n",
    "USO:\n",
    "----\n",
    "1. Reemplaza X_train, y_train, X_test, y_test con tus datos\n",
    "2. Ejecuta: python script_completo_todo_en_uno.py\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 1: LIMPIADOR DE TEXTO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class LimpiadorTexto:\n",
    "    \"\"\"Limpia texto: min√∫sculas, sin caracteres especiales, etc.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.correcciones = {\n",
    "            \"q\": \"que\",\n",
    "            \"xq\": \"porque\",\n",
    "            \"pq\": \"porque\",\n",
    "            \"tmb\": \"tambi√©n\",\n",
    "            \"tb\": \"tambi√©n\",\n",
    "            \"x\": \"por\",\n",
    "            \"d\": \"de\",\n",
    "            \"k\": \"que\",\n",
    "            \"bn\": \"bien\",\n",
    "            \"jajaja\": \"jaja\",\n",
    "            \"jeje\": \"jaja\",\n",
    "        }\n",
    "\n",
    "    def limpiar(self, texto):\n",
    "        \"\"\"Limpia un texto\"\"\"\n",
    "        if not isinstance(texto, str):\n",
    "            return \"\"\n",
    "\n",
    "        texto = texto.lower()\n",
    "        texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "        texto = re.sub(r\"\\S+@\\S+\", \"\", texto)\n",
    "        texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "        texto = re.sub(r\"#\\w+\", \"\", texto)\n",
    "        texto = re.sub(r\"\\d+\", \"\", texto)\n",
    "\n",
    "        # Corregir palabras\n",
    "        palabras = texto.split()\n",
    "        palabras = [self.correcciones.get(p, p) for p in palabras]\n",
    "        texto = \" \".join(palabras)\n",
    "\n",
    "        texto = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú\\s]\", \"\", texto)\n",
    "        texto = \" \".join(texto.split()).strip()\n",
    "\n",
    "        return texto\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 2: LEMATIZADOR\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class Lematizador:\n",
    "    \"\"\"Lematiza palabras (reduce a forma base)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lemas = {\n",
    "            \"soy\": \"ser\",\n",
    "            \"eres\": \"ser\",\n",
    "            \"es\": \"ser\",\n",
    "            \"somos\": \"ser\",\n",
    "            \"son\": \"ser\",\n",
    "            \"estoy\": \"estar\",\n",
    "            \"est√°s\": \"estar\",\n",
    "            \"est√°\": \"estar\",\n",
    "            \"est√°n\": \"estar\",\n",
    "            \"tengo\": \"tener\",\n",
    "            \"tienes\": \"tener\",\n",
    "            \"tiene\": \"tener\",\n",
    "            \"tienen\": \"tener\",\n",
    "            \"datos\": \"dato\",\n",
    "            \"textos\": \"texto\",\n",
    "            \"palabras\": \"palabra\",\n",
    "            \"modelos\": \"modelo\",\n",
    "            \"redes\": \"red\",\n",
    "        }\n",
    "\n",
    "    def lematizar(self, palabra):\n",
    "        if palabra in self.lemas:\n",
    "            return self.lemas[palabra]\n",
    "        if palabra.endswith(\"es\") and len(palabra) > 4:\n",
    "            return palabra[:-2]\n",
    "        elif palabra.endswith(\"s\") and len(palabra) > 3:\n",
    "            return palabra[:-1]\n",
    "        return palabra\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLASE 3: REPRESENTADOR TF-IDF\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RepresentadorTFIDF:\n",
    "    \"\"\"Tokeniza + Lematiza + Representa como TF-IDF\"\"\"\n",
    "\n",
    "    def __init__(self, max_features=1000, min_df=2):\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.lematizador = Lematizador()\n",
    "        self.vocabulario = {}\n",
    "        self.feature_names = []\n",
    "        self.idf_dict = {}\n",
    "\n",
    "    def _procesar(self, texto):\n",
    "        \"\"\"Tokeniza y lematiza\"\"\"\n",
    "        tokens = texto.split()\n",
    "        return [self.lematizador.lematizar(t) for t in tokens]\n",
    "\n",
    "    def fit(self, textos):\n",
    "        \"\"\"Construye vocabulario\"\"\"\n",
    "        todos_tokens = [self._procesar(t) for t in textos]\n",
    "\n",
    "        doc_freq = Counter()\n",
    "        for tokens in todos_tokens:\n",
    "            doc_freq.update(set(tokens))\n",
    "\n",
    "        tokens_validos = [t for t, f in doc_freq.items() if f >= self.min_df]\n",
    "\n",
    "        if len(tokens_validos) > self.max_features:\n",
    "            tokens_ordenados = sorted(\n",
    "                doc_freq.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            tokens_validos = [t[0] for t in tokens_ordenados[: self.max_features]]\n",
    "\n",
    "        self.vocabulario = {t: i for i, t in enumerate(tokens_validos)}\n",
    "        self.feature_names = tokens_validos\n",
    "\n",
    "        n_docs = len(textos)\n",
    "        for token in tokens_validos:\n",
    "            df = doc_freq[token]\n",
    "            self.idf_dict[token] = np.log(n_docs / df) + 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, textos):\n",
    "        \"\"\"Transforma textos a TF-IDF\"\"\"\n",
    "        X = np.zeros((len(textos), len(self.vocabulario)))\n",
    "\n",
    "        for i, texto in enumerate(textos):\n",
    "            tokens = self._procesar(texto)\n",
    "            token_counts = Counter(tokens)\n",
    "\n",
    "            for token, tf in token_counts.items():\n",
    "                if token in self.vocabulario:\n",
    "                    idx = self.vocabulario[token]\n",
    "                    idf = self.idf_dict[token]\n",
    "                    X[i, idx] = tf * idf\n",
    "\n",
    "        # Normalizaci√≥n L2\n",
    "        normas = np.sqrt(np.sum(X**2, axis=1, keepdims=True))\n",
    "        normas[normas == 0] = 1\n",
    "        X = X / normas\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, textos):\n",
    "        self.fit(textos)\n",
    "        return self.transform(textos)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCI√ìN: ENTRENAR MODELO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def entrenar_modelo(X_train, y_train, modelo_tipo=\"logistic\", verbose=True):\n",
    "    \"\"\"Entrena modelo de clasificaci√≥n\"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ü§ñ ENTRENANDO MODELO\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Tipo: {modelo_tipo}\")\n",
    "        print(f\"Muestras: {len(X_train)}\")\n",
    "\n",
    "    if modelo_tipo == \"logistic\":\n",
    "        modelo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    else:\n",
    "        modelo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"‚úÖ Modelo entrenado\\n\")\n",
    "\n",
    "    return modelo\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCI√ìN: EVALUAR MODELO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def evaluar_modelo(modelo, X_test, y_test, verbose=True):\n",
    "    \"\"\"Eval√∫a el modelo\"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"üîç EVALUANDO MODELO\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ ACCURACY: {accuracy:.4f} ({accuracy * 100:.2f}%)\\n\")\n",
    "\n",
    "        print(\"üìä MATRIZ DE CONFUSI√ìN:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "\n",
    "        print(\"\\nüìà REPORTE DETALLADO:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"y_pred\": y_pred, \"confusion_matrix\": cm}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PIPELINE COMPLETO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def pipeline_completo(\n",
    "    X_train_crudo, y_train, X_test_crudo, y_test, max_features=1000, min_df=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de inicio a fin\n",
    "\n",
    "    Args:\n",
    "        X_train_crudo: textos de entrenamiento (sin procesar)\n",
    "        y_train: etiquetas de entrenamiento\n",
    "        X_test_crudo: textos de prueba (sin procesar)\n",
    "        y_test: etiquetas de prueba\n",
    "        max_features: n√∫mero m√°ximo de palabras\n",
    "        min_df: frecuencia m√≠nima de palabra\n",
    "\n",
    "    Returns:\n",
    "        modelo entrenado, representador, resultados\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"üöÄ\" * 35)\n",
    "    print(\"PIPELINE COMPLETO - CLASIFICACI√ìN DE TEXTO\")\n",
    "    print(\"üöÄ\" * 35 + \"\\n\")\n",
    "\n",
    "    # PASO 1: Limpiar\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PASO 1: LIMPIEZA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    limpiador = LimpiadorTexto()\n",
    "\n",
    "    print(\"Limpiando X_train...\")\n",
    "    X_train_limpio = [limpiador.limpiar(texto) for texto in X_train_crudo]\n",
    "    print(f\"‚úì {len(X_train_limpio)} textos limpios\")\n",
    "\n",
    "    print(\"Limpiando X_test...\")\n",
    "    X_test_limpio = [limpiador.limpiar(texto) for texto in X_test_crudo]\n",
    "    print(f\"‚úì {len(X_test_limpio)} textos limpios\\n\")\n",
    "\n",
    "    # Mostrar ejemplo\n",
    "    print(f\"Ejemplo de limpieza:\")\n",
    "    print(f\"  Original: {X_train_crudo[0][:60]}...\")\n",
    "    print(f\"  Limpio:   {X_train_limpio[0]}\\n\")\n",
    "\n",
    "    # PASO 2: Procesar y representar\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PASO 2: TOKENIZACI√ìN + LEMATIZACI√ìN + TF-IDF\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    representador = RepresentadorTFIDF(max_features=max_features, min_df=min_df)\n",
    "\n",
    "    print(\"Procesando X_train...\")\n",
    "    X_train_procesado = representador.fit_transform(X_train_limpio)\n",
    "    print(f\"‚úì Matriz TF-IDF: {X_train_procesado.shape}\")\n",
    "    print(f\"‚úì Vocabulario: {len(representador.vocabulario)} palabras\")\n",
    "\n",
    "    print(\"\\nProcesando X_test con el MISMO representador...\")\n",
    "    X_test_procesado = representador.transform(X_test_limpio)\n",
    "    print(f\"‚úì Matriz TF-IDF: {X_test_procesado.shape}\\n\")\n",
    "\n",
    "    # PASO 3: Entrenar\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PASO 3: ENTRENAMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    modelo = entrenar_modelo(X_train_procesado, y_train, verbose=True)\n",
    "\n",
    "    # PASO 4: Evaluar\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PASO 4: EVALUACI√ìN CON X_TEST\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    resultados = evaluar_modelo(modelo, X_test_procesado, y_test, verbose=True)\n",
    "\n",
    "    # PASO 5: Mostrar predicciones\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç PREDICCIONES vs REALES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    y_pred = resultados[\"y_pred\"]\n",
    "    for i, (texto, real, pred) in enumerate(\n",
    "        zip(X_test_crudo[:10], y_test[:10], y_pred[:10]), 1\n",
    "    ):\n",
    "        estado = \"‚úÖ\" if real == pred else \"‚ùå\"\n",
    "        print(f\"\\n{i}. {estado} Real: {real} | Predicho: {pred}\")\n",
    "        print(f\"   Texto: {texto[:60]}...\")\n",
    "\n",
    "    # PASO 6: Guardar\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üíæ GUARDANDO MODELO Y REPRESENTADOR\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    with open(\"modelo_final.pkl\", \"wb\") as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    print(\"‚úì Modelo guardado: modelo_final.pkl\")\n",
    "\n",
    "    with open(\"representador_final.pkl\", \"wb\") as f:\n",
    "        pickle.dump(representador, f)\n",
    "    print(\"‚úì Representador guardado: representador_final.pkl\")\n",
    "\n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä RESUMEN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\"\"\n",
    "    ‚úÖ X_train: {len(X_train_crudo)} muestras\n",
    "    ‚úÖ X_test:  {len(X_test_crudo)} muestras\n",
    "    ‚úÖ Vocabulario: {len(representador.vocabulario)} palabras\n",
    "    ‚úÖ Accuracy: {resultados[\"accuracy\"]:.2%}\n",
    "    \n",
    "    üìÇ Archivos generados:\n",
    "       ‚Ä¢ modelo_final.pkl\n",
    "       ‚Ä¢ representador_final.pkl\n",
    "    \"\"\")\n",
    "\n",
    "    return modelo, representador, resultados\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCI√ìN: PREDECIR TEXTO NUEVO\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def predecir_texto_nuevo(texto_crudo, modelo, representador):\n",
    "    \"\"\"\n",
    "    Predice la clase de un texto nuevo\n",
    "\n",
    "    Args:\n",
    "        texto_crudo: texto sin procesar\n",
    "        modelo: modelo entrenado\n",
    "        representador: representador entrenado\n",
    "\n",
    "    Returns:\n",
    "        predicci√≥n\n",
    "    \"\"\"\n",
    "    # Limpiar\n",
    "    limpiador = LimpiadorTexto()\n",
    "    texto_limpio = limpiador.limpiar(texto_crudo)\n",
    "\n",
    "    # Representar\n",
    "    X_procesado = representador.transform([texto_limpio])\n",
    "\n",
    "    # Predecir\n",
    "    prediccion = modelo.predict(X_procesado)[0]\n",
    "\n",
    "    # Probabilidad\n",
    "    if hasattr(modelo, \"predict_proba\"):\n",
    "        probabilidad = modelo.predict_proba(X_procesado)[0]\n",
    "        confianza = probabilidad[prediccion]\n",
    "        return prediccion, confianza\n",
    "\n",
    "    return prediccion, None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: EJEMPLO DE USO\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # =========================================================================\n",
    "    # üî• AQU√ç REEMPLAZA CON TUS DATOS REALES\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"üì¶\" * 30)\n",
    "    print(\"DATOS DE EJEMPLO - REEMPLAZA CON TUS DATOS\")\n",
    "    print(\"üì¶\" * 30 + \"\\n\")\n",
    "\n",
    "    # Ejemplo: Si tienes tus datos en CSV\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Cargar datos\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    \n",
    "    X_train_crudo = df_train['columna_texto'].tolist()\n",
    "    y_train = df_train['columna_etiqueta'].tolist()\n",
    "    X_test_crudo = df_test['columna_texto'].tolist()\n",
    "    y_test = df_test['columna_etiqueta'].tolist()\n",
    "    \"\"\"\n",
    "\n",
    "    # Datos de ejemplo (REEMPLAZA ESTO)\n",
    "    X_train_crudo = [\n",
    "        \"Este producto es excelente me encant√≥\",\n",
    "        \"Muy mal servicio no lo recomiendo\",\n",
    "        \"Buena calidad precio correcto\",\n",
    "        \"Terrible experiencia p√©simo\",\n",
    "        \"Genial lo volver√≠a a comprar\",\n",
    "        \"No vale la pena muy caro\",\n",
    "        \"Excelente producto muy satisfecho\",\n",
    "        \"Horrible no funciona bien\",\n",
    "        \"Cumple con lo esperado bien\",\n",
    "        \"Decepcionante no lo compren\",\n",
    "        \"Perfecto justo lo que necesitaba\",\n",
    "        \"Mal√≠simo perd√≠ mi dinero\",\n",
    "        \"Recomendable buena compra\",\n",
    "        \"P√©sima calidad no sirve\",\n",
    "        \"Incre√≠ble super√≥ expectativas\",\n",
    "        \"No recomiendo mala experiencia\",\n",
    "        \"Muy bueno estoy contento\",\n",
    "        \"Fatal no funciona nada\",\n",
    "        \"Correcto hace su funci√≥n\",\n",
    "        \"Horrible muy decepcionado\",\n",
    "    ]\n",
    "\n",
    "    y_train = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "    X_test_crudo = [\n",
    "        \"Producto excelente muy recomendable\",\n",
    "        \"No lo compren muy malo\",\n",
    "        \"Buena opci√≥n cumple expectativas\",\n",
    "        \"P√©simo no sirve para nada\",\n",
    "        \"Genial estoy muy contento\",\n",
    "    ]\n",
    "\n",
    "    y_test = [1, 0, 1, 0, 1]\n",
    "\n",
    "    print(f\"X_train: {len(X_train_crudo)} muestras\")\n",
    "    print(f\"X_test:  {len(X_test_crudo)} muestras\")\n",
    "    print(f\"Clases:  {np.unique(y_train)}\\n\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # EJECUTAR PIPELINE COMPLETO\n",
    "    # =========================================================================\n",
    "\n",
    "    modelo, representador, resultados = pipeline_completo(\n",
    "        X_train_crudo,\n",
    "        y_train,\n",
    "        X_test_crudo,\n",
    "        y_test,\n",
    "        max_features=100,  # Aumenta con m√°s datos\n",
    "        min_df=1,  # Cambia a 2 con m√°s datos\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # EJEMPLO DE PREDICCI√ìN CON TEXTO NUEVO\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîÆ PREDICCI√ìN CON TEXTOS NUEVOS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    textos_nuevos = [\n",
    "        \"Este producto es incre√≠ble lo recomiendo\",\n",
    "        \"Horrible experiencia no funciona\",\n",
    "        \"Producto correcto cumple funci√≥n\",\n",
    "    ]\n",
    "\n",
    "    for i, texto in enumerate(textos_nuevos, 1):\n",
    "        pred, conf = predecir_texto_nuevo(texto, modelo, representador)\n",
    "        sentimiento = \"POSITIVO ‚úÖ\" if pred == 1 else \"NEGATIVO ‚ùå\"\n",
    "\n",
    "        print(f\"\\n{i}. Texto: {texto}\")\n",
    "        print(f\"   Predicci√≥n: {sentimiento}\")\n",
    "        if conf:\n",
    "            print(f\"   Confianza: {conf:.2%}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # INSTRUCCIONES FINALES\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ ¬°COMPLETADO!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    Para usar con TUS datos:\n",
    "    \n",
    "    1. Reemplaza las l√≠neas 324-347 con tus datos reales:\n",
    "    \n",
    "       import pandas as pd\n",
    "       \n",
    "       df = pd.read_csv('tus_datos.csv')\n",
    "       X_train_crudo = df['columna_texto'].tolist()\n",
    "       y_train = df['columna_etiquetas'].tolist()\n",
    "       # etc...\n",
    "    \n",
    "    2. Ejecuta el script:\n",
    "       \n",
    "       python script_completo_todo_en_uno.py\n",
    "    \n",
    "    3. Usa los archivos generados:\n",
    "       \n",
    "       ‚Ä¢ modelo_final.pkl\n",
    "       ‚Ä¢ representador_final.pkl\n",
    "    \n",
    "    ¬°Listo! üéâ\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1742314576802,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "cJ9KMVyLmaX0",
    "outputId": "df20535f-365b-439f-db5a-0ae5270d0bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o y_test: 5\n",
      "Tama√±o y_pred: 5\n"
     ]
    }
   ],
   "source": [
    "X_test_procesado = representador.transform(X_test_crudo)\n",
    "\n",
    "\n",
    "y_pred = modelo.predict(X_test_procesado)\n",
    "\n",
    "\n",
    "print(\"Tama√±o y_test:\", len(y_test))\n",
    "print(\"Tama√±o y_pred:\", len(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Reporte de clasificaci√≥n:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "print(\n",
    "    \"Reporte de clasificaci√≥n:\\n\",\n",
    "    classification_report(y_test, y_pred, zero_division=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8EMg20rzGVw"
   },
   "source": [
    "**Guardar el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1742314634787,
     "user": {
      "displayName": "Carlos Bustillo",
      "userId": "14895763228834044971"
     },
     "user_tz": 180
    },
    "id": "mP9pEc3ezIey",
    "outputId": "4aaff043-6a9e-49f5-ec0a-657b9b879b82"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as Usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paquetes/Distribuciones detectadas:\n",
      " Pygments\n",
      " asttokens\n",
      " charset-normalizer\n",
      " comm\n",
      " cramjam\n",
      " cython_runtime\n",
      " debugpy\n",
      " decorator\n",
      " executing\n",
      " fastparquet\n",
      " fsspec\n",
      " ipykernel\n",
      " ipython\n",
      " ipywidgets\n",
      " jedi\n",
      " joblib\n",
      " jupyter_client\n",
      " jupyter_core\n",
      " numpy\n",
      " packaging\n",
      " pandas\n",
      " parso\n",
      " platformdirs\n",
      " prompt_toolkit\n",
      " psutil\n",
      " pure_eval\n",
      " pydev_ipython\n",
      " pydevconsole\n",
      " pydevd\n",
      " pydevd_file_utils\n",
      " pydevd_plugins\n",
      " pydevd_tracing\n",
      " python-dateutil\n",
      " pytz\n",
      " pyzmq\n",
      " scikit-learn\n",
      " scipy\n",
      " six\n",
      " stack-data\n",
      " threadpoolctl\n",
      " tornado\n",
      " traitlets\n",
      " vscode\n",
      " wcwidth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from importlib import metadata\n",
    "\n",
    "mods = {m.split(\".\")[0] for m in sys.modules.keys() if m and not m.startswith(\"_\")}\n",
    "stdlib = getattr(sys, \"stdlib_module_names\", set())\n",
    "third_party = sorted(m for m in mods if m and (m not in stdlib))\n",
    "\n",
    "pkg_map = {}\n",
    "try:\n",
    "    pkg_map = metadata.packages_distributions()  # top_mod -> [dist]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "dists = sorted({(pkg_map.get(m) or [None])[0] or m for m in third_party})\n",
    "print(\"Paquetes/Distribuciones detectadas:\\n\", \"\\n \".join(dists))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Referencias\n",
    "- [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- [Classification of text documents using sparse features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html)\n",
    "- Ejemplo de Entrenamiento y selecci√≥n de Modelo de machine learning entre varios modelos <https://joserzapata.github.io/post/ciencia-datos-proyecto-python/6-model_selection/>\n",
    "- https://joserzapata.github.io/courses/python-ciencia-datos/python/\n",
    "- https://joserzapata.github.io/courses/python-ciencia-datos/pandas/\n",
    "- https://joserzapata.github.io/courses/python-ciencia-datos/machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docente: [Jose R. Zapata](https://joserzapata.github.io)\n",
    "\n",
    "- [https://joserzapata.github.io/](https://joserzapata.github.io/)\n",
    "- [https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/](https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO2onnY6t+Q8frrcGj/XGl2",
   "collapsed_sections": [
    "TcXzT5msVNnF",
    "yWXYzke6qtI3"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "template-data-science-container",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
